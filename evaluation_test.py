"""RAG íŒŒì´í”„ë¼ì¸ ì‹¤ì œ í‰ê°€ í…ŒìŠ¤íŠ¸"""

import sys

sys.path.append("src")

from src.rag_evaluator import RAGEvaluator
from src.rag import SmartStoreRAG
from src.utils import get_api_key
from openai import OpenAI


def evaluate_rag_pipeline():
    """ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ í‰ê°€"""
    try:
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        api_key = get_api_key()
        rag = SmartStoreRAG(api_key)
        client = OpenAI(api_key=api_key)
        evaluator = RAGEvaluator(openai_client=client)

        # ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
        question = input("í‰ê°€í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not question:
            question = "ë¯¸ì„±ë…„ìë„ íŒë§¤íšŒì› ë“±ë¡ì´ ê°€ëŠ¥í•œê°€ìš”?"  # ê¸°ë³¸ ì§ˆë¬¸
            print(f"ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©: {question}")

        print(f"\nğŸ” ì§ˆë¬¸: {question}")

        # RAG ê²€ìƒ‰ ì‹¤í–‰
        search_results = rag.vector_db.search(question, top_k=5)

        # ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
        metrics = evaluator.evaluate_search_performance(search_results, similarity_threshold=0.1, top_k=5)

        print("\nğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€:")
        print(f"   ì‚¬ìš©ë¥ : {metrics['usage_ratio']:.2%}")
        print(f"   í‰ê·  ìœ ì‚¬ë„: {metrics['avg_similarity']:.3f}")
        print(f"   ì‚¬ìš© ë¬¸ì„œ: {metrics['used_docs']}/{metrics['total_docs']}")

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ
        print("\nğŸ“‹ ê²€ìƒ‰ëœ ë¬¸ì„œ:")
        for i, result in enumerate(search_results, 1):
            score = result.get("similarity_score", 0)
            question_text = result.get("question", "N/A")[:50]
            print(f"   {i}. {question_text}... (ìœ ì‚¬ë„: {score:.3f})")

        # ìƒì„± ì„±ëŠ¥ í‰ê°€
        print("\nğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")

        # RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘
        full_answer = ""
        for chunk in rag.stream_response(question):
            if chunk["type"] == "answer_chunk":
                full_answer += chunk["content"]
            elif chunk["type"] == "answer":
                full_answer = chunk["content"]

        if full_answer:
            # LLM ìê¸° í‰ê°€
            quality_scores = evaluator.evaluate_answer_quality(question, full_answer)

            print("\nğŸ“ ìƒì„±ëœ ë‹µë³€:")
            print(f"   {full_answer}...")

            print("\nâ­ ë‹µë³€ í’ˆì§ˆ í‰ê°€:")
            reasons = quality_scores.get("reasons", {})
            print(f"   ê´€ë ¨ì„±: {quality_scores['relevance']}/5 - {reasons.get('relevance', 'N/A')}")
            print(f"   ì™„ì„±ë„: {quality_scores['completeness']}/5 - {reasons.get('completeness', 'N/A')}")
            print(f"   ì •í™•ì„±: {quality_scores['accuracy']}/5 - {reasons.get('accuracy', 'N/A')}")
            print(f"   ì „ì²´: {quality_scores['overall']}/5")
        else:
            print("\nâŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    evaluate_rag_pipeline()
